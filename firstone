import os, random
import numpy as np
import pandas as pd

import matplotlib.pyplot as plt
from matplotlib import ticker
import seaborn as sns

from keras import backend as K
from keras.models import Sequential
from keras.layers import Input, Dropout, Flatten, Conv2D, MaxPooling2D, Dense, Activation
from keras.optimizers import RMSprop
from keras.callbacks import ModelCheckpoint, Callback, EarlyStopping
from keras.utils import np_utils
from keras.preprocessing.image import ImageDataGenerator
# For restoring pickled data
import pickle
# Suffle data set



# Directory containing extracted CROHME data
TRAIN_DIR = './train'
TEST_DIR  = './valid'

classes = [ '0', '1', '2', '3', '4', '5', '6', '7', '8', '9',
            '(', ')', '[', ']', '+', '-', '/', '=', '\geq', '\gt', '\leq', '\lt', '\\neq', '\\times', '\div']
n_classes = len(classes)


batch_size = 15000 #we'll be training the NN with 100 images at a time

train_datagen = ImageDataGenerator(
        rescale=1./255
        )


test_datagen = ImageDataGenerator(rescale=1./255)



train_generator = train_datagen.flow_from_directory(
        './train',
        target_size=(150, 150),
        batch_size=32,
        class_mode='categorical')

validation_generator = test_datagen.flow_from_directory(
        './valid',
        target_size=(150, 150),
        batch_size=32,
        class_mode='categorical')


optimizer = RMSprop(lr=1e-3)
objective = 'categorical_crossentropy'

def mathsymbol():
    model = Sequential()

    model.add(Conv2D(32, 3, padding='same', input_shape=(150,150,3), activation='relu'))
    model.add(Conv2D(32, 3, padding='same', activation='relu'))
    model.add(MaxPooling2D(pool_size=(2, 2), data_format="channels_first"))
    model.add(Conv2D(64,3,padding='same', activation = 'relu'))
    model.add(Conv2D(64,3,padding='same', activation = 'relu'))
    model.add(MaxPooling2D(pool_size=(2,2),data_format = "channels_first"))
    #print("First layer...")
    model.add(Flatten())
    model.add(Dense(47))
    
    model.add(Activation('softmax'))
    print("Compiling model...")
    model.compile(loss=objective, optimizer=optimizer, metrics=['accuracy'])
    return model


print("Creating model:")
model = mathsymbol()

class LossHistory(Callback):
    def on_train_begin(self, logs={}):
        self.losses = []
        self.val_losses = []

    def on_epoch_end(self, batch, logs={}):
        self.losses.append(logs.get('loss'))
        self.val_losses.append(logs.get('val_loss'))

early_stopping = EarlyStopping(monitor='val_loss', patience=3, verbose=1, mode='auto')        
       
best = 0.0


def run_mathsymbol():

	history = LossHistory()
	print("running model...")
	for i in range(20):
		print(i+1)
		model.fit_generator(train_generator,steps_per_epoch = 1000,epochs=1,validation_data=validation_generator,validation_steps = 400) 
		predctions = model.predict(test_datagen)
		print(predictions)
		if predictions>best:
			best = predictions
			print("running on test set")
			model.save_weights('./weights.h5')
		print("Best so far:", best)






